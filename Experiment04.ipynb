{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oSruI4IajU9u"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LENGTH = 5\n",
        "BATCH_SIZE = 64\n",
        "HIDDEN_SIZE = 64\n",
        "EMBED_DIM = 100\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 20\n",
        "\n",
        "def load_and_process_data(filename):\n",
        "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read().lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "tokens = load_and_process_data(\"poems-100.csv\")"
      ],
      "metadata": {
        "id": "1Jn6POzmyaoy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(tokens))\n",
        "vocab_size = len(vocab)\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "ix_to_word = {i: word for word, i in word_to_ix.items()}\n",
        "\n",
        "inputs = []\n",
        "targets = []\n",
        "\n",
        "for i in range(len(tokens) - SEQ_LENGTH):\n",
        "    seq_in = tokens[i : i + SEQ_LENGTH]\n",
        "    seq_out = tokens[i + SEQ_LENGTH]\n",
        "    inputs.append([word_to_ix[w] for w in seq_in])\n",
        "    targets.append(word_to_ix[seq_out])\n",
        "\n",
        "X_tensor = torch.tensor(inputs, dtype=torch.long)\n",
        "y_tensor = torch.tensor(targets, dtype=torch.long)\n",
        "\n",
        "X_one_hot = torch.zeros(len(inputs), SEQ_LENGTH, vocab_size)\n",
        "for i in range(len(inputs)):\n",
        "    for t in range(SEQ_LENGTH):\n",
        "        X_one_hot[i, t, inputs[i][t]] = 1.0\n",
        "\n",
        "dataset_onehot = TensorDataset(X_one_hot, y_tensor)\n",
        "loader_onehot = DataLoader(dataset_onehot, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "dataset_embed = TensorDataset(X_tensor, y_tensor)\n",
        "loader_embed = DataLoader(dataset_embed, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "Uaq7IcN3ziLy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN_OneHot(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(RNN_OneHot, self).__init__()\n",
        "        self.rnn = nn.RNN(vocab_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        out = out[:, -1, :]\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "m4xoB3tGzqhY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_OneHot(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(LSTM_OneHot, self).__init__()\n",
        "        self.lstm = nn.LSTM(vocab_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, (hn, cn) = self.lstm(x)\n",
        "        out = out[:, -1, :]\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "TiG51MOMzu0E"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN_Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_size):\n",
        "        super(RNN_Embedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn = nn.RNN(embed_dim, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, _ = self.rnn(x)\n",
        "        out = out[:, -1, :]\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "b9aIp9SJzxIj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_size):\n",
        "        super(LSTM_Embedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, (hn, cn) = self.lstm(x)\n",
        "        out = out[:, -1, :]\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "Hn5DJT90zzhO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloader, criterion, optimizer, epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for x_batch, y_batch in dataloader:\n",
        "            outputs = model(x_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader):.4f}\")\n",
        "\n",
        "def generate_text(model, seed_text, next_words=50, is_one_hot=False):\n",
        "    model.eval()\n",
        "    words = seed_text.lower().split()\n",
        "\n",
        "    for _ in range(next_words):\n",
        "        current_seq = words[-SEQ_LENGTH:]\n",
        "        input_indices = [word_to_ix[w] for w in current_seq if w in word_to_ix]\n",
        "\n",
        "        if len(input_indices) < SEQ_LENGTH:\n",
        "            break\n",
        "\n",
        "        input_seq = torch.tensor(input_indices).unsqueeze(0)\n",
        "\n",
        "        if is_one_hot:\n",
        "            one_hot_input = torch.zeros(1, SEQ_LENGTH, vocab_size)\n",
        "            for t in range(SEQ_LENGTH):\n",
        "                one_hot_input[0, t, input_seq[0, t]] = 1.0\n",
        "            input_seq = one_hot_input\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(input_seq)\n",
        "            predicted_ix = torch.argmax(output, dim=1).item()\n",
        "\n",
        "        words.append(ix_to_word[predicted_ix])\n",
        "\n",
        "    return \" \".join(words)"
      ],
      "metadata": {
        "id": "vyR6WBfpz2fP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training RNN One-Hot...\")\n",
        "rnn_oh = RNN_OneHot(vocab_size, HIDDEN_SIZE)\n",
        "opt_rnn_oh = optim.Adam(rnn_oh.parameters(), lr=LEARNING_RATE)\n",
        "train_model(rnn_oh, loader_onehot, nn.CrossEntropyLoss(), opt_rnn_oh, EPOCHS)\n",
        "\n",
        "print(\"Training LSTM One-Hot...\")\n",
        "lstm_oh = LSTM_OneHot(vocab_size, HIDDEN_SIZE)\n",
        "opt_lstm_oh = optim.Adam(lstm_oh.parameters(), lr=LEARNING_RATE)\n",
        "train_model(lstm_oh, loader_onehot, nn.CrossEntropyLoss(), opt_lstm_oh, EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqVcEFOZz8mu",
        "outputId": "4f0ea0ec-579b-4367-e00a-653ebf426447"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training RNN One-Hot...\n",
            "Epoch 5, Loss: 5.7891\n",
            "Epoch 10, Loss: 4.6250\n",
            "Epoch 15, Loss: 3.5483\n",
            "Epoch 20, Loss: 2.6350\n",
            "Training LSTM One-Hot...\n",
            "Epoch 5, Loss: 6.1563\n",
            "Epoch 10, Loss: 5.1319\n",
            "Epoch 15, Loss: 4.0177\n",
            "Epoch 20, Loss: 3.0138\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training RNN Embedding...\")\n",
        "rnn_emb = RNN_Embedding(vocab_size, EMBED_DIM, HIDDEN_SIZE)\n",
        "opt_rnn_emb = optim.Adam(rnn_emb.parameters(), lr=LEARNING_RATE)\n",
        "train_model(rnn_emb, loader_embed, nn.CrossEntropyLoss(), opt_rnn_emb, EPOCHS)\n",
        "\n",
        "print(\"Training LSTM Embedding...\")\n",
        "lstm_emb = LSTM_Embedding(vocab_size, EMBED_DIM, HIDDEN_SIZE)\n",
        "opt_lstm_emb = optim.Adam(lstm_emb.parameters(), lr=LEARNING_RATE)\n",
        "train_model(lstm_emb, loader_embed, nn.CrossEntropyLoss(), opt_lstm_emb, EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ui-9EmWo0DUY",
        "outputId": "ea2b3b27-4d17-4587-a3c8-c406ee8a6c0c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training RNN Embedding...\n",
            "Epoch 5, Loss: 5.4800\n",
            "Epoch 10, Loss: 4.3378\n",
            "Epoch 15, Loss: 3.3797\n",
            "Epoch 20, Loss: 2.6403\n",
            "Training LSTM Embedding...\n",
            "Epoch 5, Loss: 5.6586\n",
            "Epoch 10, Loss: 4.4942\n",
            "Epoch 15, Loss: 3.4390\n",
            "Epoch 20, Loss: 2.5628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D3Dza3R13VRY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}